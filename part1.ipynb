{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54504d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for epsilon-greedy Algorithm:\n",
      "2368.7 with parameter epsilon as 0.1\n",
      "2461.855 with parameter epsilon as 0.5\n",
      "2553.58 with parameter epsilon as 0.9\n",
      "results for UCB Algorithm:\n",
      "3415.575 with parameter c as 1\n",
      "2952.9 with parameter c as 5\n",
      "2768.92 with parameter c as 10\n",
      "results for TS Algorithm:\n",
      "3486.185 with parameter a,b as [[1, 1], [1, 1], [1, 1]]\n",
      "3493.595 with parameter a,b as [[601, 401], [401, 601], [2, 3]]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Cython import wraparound, boundscheck\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cython.parallel as parallel\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, path):\n",
    "        self.data=None\n",
    "        self.table=None\n",
    "        self.arm_num=None\n",
    "        self.means=None\n",
    "        self.optArm=None\n",
    "        self.sampler=None\n",
    "        self.range=None\n",
    "        self.path=path\n",
    "        self.load_data(path)\n",
    "\n",
    "    def load_data(self,path):\n",
    "        self.data=pd.read_csv(path)\n",
    "        if 'arm1' in self.data.columns.values:\n",
    "            self.load_data_gen()\n",
    "        else:\n",
    "            self.load_data_movie()\n",
    "\n",
    "    def load_data_gen(self):\n",
    "        path=self.path\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.arm_num = self.data.columns.size\n",
    "        self.range=self.data[self.data.columns[0]].max()-self.data[self.data.columns[0]].min()\n",
    "        self.means = [0] * self.arm_num\n",
    "        for i in range(self.arm_num):\n",
    "            self.means[i] = self.data[self.data.columns[i]].mean()\n",
    "        self.optArm = np.argmax(self.means)\n",
    "\n",
    "        # 0,1,2,...\n",
    "        self.table = np.zeros([self.arm_num, self.data.values.max() + 1, self.arm_num])\n",
    "        for s_idx, source_arm in enumerate(self.data.columns):\n",
    "            for a_idx, aim_arm in enumerate(self.data.columns):\n",
    "                for i in range(self.data.values.max() + 1):\n",
    "                    if source_arm == aim_arm:\n",
    "                        self.table[s_idx][i][a_idx] = i\n",
    "                    else:\n",
    "                        p_reward = self.data[aim_arm][self.data[source_arm] == i].mean()\n",
    "                        if p_reward > 1:\n",
    "                            self.table[s_idx][i][a_idx] = 1\n",
    "                        else:\n",
    "                            self.table[s_idx][i][a_idx] = p_reward\n",
    "        self.sampler=self.sample_gen\n",
    "\n",
    "    def load_data_movie(self):\n",
    "        #csv data\n",
    "        self.arm_num=self.data['genre_col'].max()+1\n",
    "        self.range=self.data['Rating'].max()-self.data['Rating'].min()\n",
    "        self.means=[0]*self.arm_num\n",
    "        for i in range(self.arm_num):\n",
    "            self.means[i]=self.data[self.data['genre_col']==i]['Rating'].mean()\n",
    "        self.optArm=np.argmax(self.means)\n",
    "        #0,1,2,...\n",
    "        self.table = np.zeros([self.arm_num, self.data['Rating'].max()-self.data['Rating'].min()+1, self.arm_num])\n",
    "        for source in range(self.arm_num):\n",
    "            for score in range(self.data['Rating'].min(),self.data['Rating'].max()+1):\n",
    "                users=set(self.data[(self.data['genre_col'] == source) & (self.data['Rating'] == score)]['UserID'])\n",
    "                self.table[source][:,source]=np.arange(self.data['Rating'].min(), self.data['Rating'].max() + 1)\n",
    "                for aim in range(self.arm_num):\n",
    "                    if aim==source:\n",
    "                        continue\n",
    "                    temp=self.data[self.data['genre_col'] == aim & self.data['UserID'].isin(users)]['Rating']\n",
    "                    self.table[source][score-self.data['Rating'].min()][aim]=temp.mean()\n",
    "        self.sampler =self.sample_movie\n",
    "\n",
    "    def sample_movie(self,choose):\n",
    "        v= self.data[self.data['genre_col'] == choose]['Rating'].sample(n=1, replace=True)\n",
    "        return v.values[0]\n",
    "\n",
    "    def sample_gen(self, choose):\n",
    "        reward = self.data[self.data.columns[choose]].sample(n=1, replace=True)\n",
    "        return reward.values[0]\n",
    "\n",
    "actual_theta = [0.7, 0.5, 0.4]\n",
    "N = 5000\n",
    "trial_times = 200\n",
    "arms_part1 = [1, 2, 3]\n",
    "\n",
    "GREEDY_epsilon = [0.1, 0.5, 0.9]\n",
    "UCB_c = [1, 5, 10]\n",
    "TS_ab = [[[1, 1], [1, 1], [1, 1]],\n",
    "         [[601, 401], [401, 601], [2, 3]]]\n",
    "\n",
    "greedy_regret = np.zeros(N+1)\n",
    "TS_regret = np.zeros(N+1)\n",
    "UCB_regret = np.zeros(N+1)\n",
    "dependent_UCB_regret = np.zeros(N+1)\n",
    "dependent_TS_regret = np.zeros(N+1)\n",
    "TS_Gauss_regret=np.zeros(N+1)\n",
    "class P1:\n",
    "\n",
    "    def __init__(self, depend_data_path=None):\n",
    "        self.path = depend_data_path\n",
    "        if depend_data_path is not None:\n",
    "            self.loader=Loader(depend_data_path)\n",
    "            self.actual_theta=self.loader.means\n",
    "            self.optArm=self.loader.optArm\n",
    "            self.arm_num=self.loader.arm_num\n",
    "            self.sampler=self.loader.sampler\n",
    "            self.table=self.loader.table\n",
    "            self.range=self.loader.range\n",
    "        else:\n",
    "            self.loader=None\n",
    "            self.table=None\n",
    "            self.range = 1\n",
    "            self.actual_theta=actual_theta\n",
    "            self.optArm=np.argmax(self.actual_theta)\n",
    "            self.arm_num = len(self.actual_theta)\n",
    "            self.sampler=self.independ_reward\n",
    "\n",
    "    def independ_reward(self, choose):\n",
    "        # choose: 1,2,3\n",
    "        probability = self.actual_theta[choose]\n",
    "        if random.uniform(0, 1) < probability:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def e_Greedy(self, N, epsilon):\n",
    "        # initialize\n",
    "        # Notice we set index start from 1\n",
    "        theta = [0]*self.arm_num\n",
    "        count =[0]*self.arm_num\n",
    "        total_reward = 0\n",
    "        greedy_current_regret = np.zeros(N+1)\n",
    "\n",
    "        for t in range(1, N + 1):\n",
    "            # I_t=1,2,3...\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                I_t = random.randint(0, self.arm_num-1)\n",
    "            else:\n",
    "                I_t = np.argmax(theta)\n",
    "                if I_t == 0:\n",
    "                    I_t = 1\n",
    "\n",
    "            count[I_t] += 1\n",
    "            r = self.sampler(I_t)\n",
    "            total_reward += r\n",
    "            theta[I_t] += (1 / count[I_t]) * (r - theta[I_t])\n",
    "            if t==0:\n",
    "                greedy_current_regret[t] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                greedy_current_regret[t] = greedy_current_regret[t-1] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        global greedy_regret\n",
    "        greedy_regret += greedy_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def Ucb(self, N, c):\n",
    "        # note the index start from 0\n",
    "        count = [0]* self.arm_num\n",
    "        theta = [0]* self.arm_num\n",
    "        total_reward = 0\n",
    "        UCB_current_regret = np.zeros(N+1)\n",
    "\n",
    "        # initialize\n",
    "        for t in range(self.arm_num):\n",
    "            I_t = t\n",
    "            count[I_t] = 1\n",
    "            theta[I_t] = self.sampler(I_t)\n",
    "            if t==0:\n",
    "                UCB_current_regret[t] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                UCB_current_regret[t] = UCB_current_regret[t-1] + actual_theta[self.optArm] - actual_theta[I_t]\n",
    "\n",
    "        for t in range(self.arm_num, N + 1):\n",
    "            # select and pull arm\n",
    "            I_t =np.argmax([theta[j] + c * math.sqrt(2 * math.log(t) / count[j]) for j in range(self.arm_num)])\n",
    "\n",
    "            count[I_t] += 1\n",
    "            r = self.sampler(I_t)\n",
    "            total_reward += r\n",
    "            theta[I_t] += (r - theta[I_t]) / count[I_t]\n",
    "            UCB_current_regret[t] = UCB_current_regret[t - 1] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        global UCB_regret\n",
    "        UCB_regret += UCB_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def depend_Ucb(self, N, c):\n",
    "        table=self.table\n",
    "        arm_num=self.arm_num\n",
    "\n",
    "        count = np.array([0]*arm_num)\n",
    "        theta = np.array([0.]*arm_num)\n",
    "        ucb_idx=dict(zip(range(arm_num),[np.inf]*arm_num))\n",
    "        ave_pseudo_reward=np.array([[np.inf]*arm_num]*arm_num)\n",
    "        sum_pseudo_reward=np.array([[0.]*arm_num]*arm_num)\n",
    "        d_UCB_current_regret = np.zeros(N+1)\n",
    "\n",
    "        total_reward = 0\n",
    "        for t in range(N):\n",
    "            if t<arm_num:\n",
    "                choose=t\n",
    "            else:\n",
    "                S_bool=(count>=(float(t-1)/arm_num))\n",
    "                k_emp_reward=np.max(theta[S_bool])\n",
    "                k_emp=np.where(theta==k_emp_reward)[0][0]\n",
    "                comp_set=set()\n",
    "                comp_set.add(k_emp)\n",
    "                min_phi = np.min(ave_pseudo_reward[:, S_bool], axis=1)\n",
    "                for k in range(arm_num):\n",
    "                    if min_phi[k]>= k_emp_reward:\n",
    "                        comp_set.add(k)\n",
    "\n",
    "                comp_idx={ind: ucb_idx[ind] for ind in comp_set}\n",
    "                choose=max(comp_idx.items(),key=operator.itemgetter(1))[0]\n",
    "            # print(t,choose)\n",
    "            if t==0:\n",
    "                d_UCB_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "            else:\n",
    "                d_UCB_current_regret[t+1] = d_UCB_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "\n",
    "            reward=self.sampler(choose)\n",
    "            count[choose]+=1\n",
    "            theta[choose]+=((reward-theta[choose])/count[choose])\n",
    "\n",
    "            for arm in range(arm_num):\n",
    "                if (count[arm] > 0):\n",
    "                    ucb_idx[arm] = theta[arm] + c * np.sqrt(2 * np.log(t + 1) / count[arm])\n",
    "\n",
    "            # pseudoReward=table[choose][reward]\n",
    "            pseudoReward = table[choose][reward - 1,:]\n",
    "            sum_pseudo_reward[:, choose] = sum_pseudo_reward[:, choose]+ pseudoReward\n",
    "            ave_pseudo_reward[:, choose] = np.divide(sum_pseudo_reward[:, choose], count[choose])\n",
    "\n",
    "            ave_pseudo_reward[np.arange(arm_num),np.arange(arm_num)]=theta\n",
    "\n",
    "            total_reward+=reward\n",
    "        global dependent_UCB_regret\n",
    "        dependent_UCB_regret += d_UCB_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def TS_sample_Gauss(self,theta,count,beta):\n",
    "        std=np.sqrt(float(beta)/(count+1))\n",
    "        return np.random.normal(theta,std)\n",
    "\n",
    "\n",
    "    def TS_arm_choose_beta(self, ab):\n",
    "        theta = [0 for i in ab]\n",
    "        for i, (a, b) in enumerate(ab):\n",
    "            theta[i] = np.random.beta(a, b)\n",
    "        return np.argmax(theta)\n",
    "\n",
    "    def TS(self, N, ab_original):\n",
    "\n",
    "        total_reward = 0\n",
    "        # ab idx start from 0\n",
    "        ab = copy.deepcopy(ab_original)\n",
    "        TS_current_regret = np.zeros(N+1)\n",
    "        for t in range(N):\n",
    "            I_t = self.TS_arm_choose_beta(ab)\n",
    "            # print(I_t)\n",
    "            # update distribution\n",
    "            r = self.sampler(I_t)\n",
    "            ab[I_t][0] += r\n",
    "            ab[I_t][1] += (1 - r)\n",
    "            total_reward += r\n",
    "            if t==0:\n",
    "                TS_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                TS_current_regret[t+1] = TS_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        # compute the expectation!!\n",
    "        # result = []\n",
    "        # for j in arms:\n",
    "        #     result.append(ab[j - 1][0] / (ab[j - 1][0] + ab[j - 1][1]))\n",
    "        # print(choose_first_cluster)\n",
    "        global TS_regret\n",
    "        TS_regret += TS_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def TS_Gauss(self, N, beta):\n",
    "        arm_num=self.arm_num\n",
    "        total_reward = 0\n",
    "        # ab idx start from 0\n",
    "\n",
    "        TS_G_current_regret = np.zeros(N + 1)\n",
    "        theta = np.array([0.] * arm_num)\n",
    "        count = np.array([0] * arm_num)\n",
    "        for t in range(N):\n",
    "            I_t=np.argmax(self.TS_sample_Gauss(theta,count,beta))\n",
    "            # print(I_t)\n",
    "            # update distribution\n",
    "            r = self.sampler(I_t)\n",
    "            count[I_t] += 1\n",
    "            theta[I_t] += ((r - theta[I_t]) / count[I_t])\n",
    "            total_reward += r\n",
    "            if t==0:\n",
    "                TS_G_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                TS_G_current_regret[t+1] = TS_G_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        # compute the expectation!!\n",
    "        # result = []\n",
    "        # for j in arms:\n",
    "        #     result.append(ab[j - 1][0] / (ab[j - 1][0] + ab[j - 1][1]))\n",
    "        # print(choose_first_cluster)\n",
    "        global TS_Gauss_regret\n",
    "        TS_Gauss_regret += TS_G_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    # TODO\n",
    "    def depend_TS(self, N, beta):\n",
    "        table = self.table\n",
    "        arm_num = self.arm_num\n",
    "\n",
    "        theta = np.array([0.] * arm_num)\n",
    "        count = np.array([0] * arm_num)\n",
    "\n",
    "        ave_pseudo_reward = np.array([[np.inf] * arm_num] * arm_num)\n",
    "        sum_pseudo_reward = np.array([[0.] * arm_num] * arm_num)\n",
    "        d_TS_current_regret = np.zeros(N + 1)\n",
    "\n",
    "        total_reward = 0\n",
    "        for t in range(N):\n",
    "            if t<arm_num:\n",
    "                choose=t\n",
    "            else:\n",
    "                S_bool = (count >= (float(t - 1) / arm_num))\n",
    "\n",
    "                k_emp_reward = np.max(theta[S_bool])\n",
    "                k_emp = np.where(theta == k_emp_reward)[0][0]\n",
    "                comp_set = set()\n",
    "                comp_set.add(k_emp)\n",
    "                min_phi = np.min(ave_pseudo_reward[:, S_bool], axis=1)\n",
    "                for k in range(arm_num):\n",
    "                    if min_phi[k] >= k_emp_reward:\n",
    "                        comp_set.add(k)\n",
    "\n",
    "                sample=self.TS_sample_Gauss(theta,count,beta)\n",
    "                comp_idx = {ind: sample[ind] for ind in comp_set}\n",
    "                choose = max(comp_idx.items(), key=operator.itemgetter(1))[0]\n",
    "            # print(t,choose)\n",
    "            if t == 0:\n",
    "                d_TS_current_regret[t + 1] = self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "            else:\n",
    "                d_TS_current_regret[t + 1] = d_TS_current_regret[t] + self.actual_theta[self.optArm] - \\\n",
    "                                              self.actual_theta[choose]\n",
    "\n",
    "            reward = self.sampler(choose)\n",
    "            count[choose] += 1\n",
    "            theta[choose] += ((reward - theta[choose]) / count[choose])\n",
    "\n",
    "            # pseudoReward=table[choose][reward]\n",
    "            pseudoReward = table[choose][reward - 1, :]\n",
    "            sum_pseudo_reward[:, choose] = sum_pseudo_reward[:, choose] + pseudoReward\n",
    "            ave_pseudo_reward[:, choose] = np.divide(sum_pseudo_reward[:, choose], count[choose])\n",
    "\n",
    "            ave_pseudo_reward[np.arange(arm_num), np.arange(arm_num)] = theta\n",
    "\n",
    "            total_reward += reward\n",
    "        global dependent_TS_regret\n",
    "        dependent_TS_regret += d_TS_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    @boundscheck(False)\n",
    "    @wraparound(False)\n",
    "    def result(self, function_idx):\n",
    "        function = ['epsilon-greedy', 'UCB', 'TS', 'D-UCB','TS-Gauss', 'D-TS']\n",
    "        print(\"results for\", function[function_idx - 1], \"Algorithm:\")\n",
    "        para = []\n",
    "        func = None\n",
    "        if function_idx == 1:\n",
    "            para = GREEDY_epsilon\n",
    "            func = self.e_Greedy\n",
    "            parameter = \"epsilon\"\n",
    "        elif function_idx == 2:\n",
    "            para = UCB_c\n",
    "            func = self.Ucb\n",
    "            parameter = \"c\"\n",
    "        elif function_idx == 3:\n",
    "            para = TS_ab\n",
    "            func = self.TS\n",
    "            parameter = \"a,b\"\n",
    "        elif function_idx == 4:\n",
    "            para = UCB_c\n",
    "            func = self.depend_Ucb\n",
    "            parameter = \"c\"\n",
    "        elif function_idx == 5:\n",
    "            para = [self.range]\n",
    "            func = self.TS_Gauss\n",
    "            parameter = \"Beta\"\n",
    "        elif function_idx == 6:\n",
    "            para = [self.range]\n",
    "            func = self.depend_TS\n",
    "            parameter = \"Beta\"\n",
    "\n",
    "        for p in para:\n",
    "            result = 0.0\n",
    "            for trial in parallel.prange(trial_times, nogil=True,schedule=\"static\", chunksize=1):\n",
    "                result += func(N, p)\n",
    "            result /= trial_times\n",
    "            print(result, \"with parameter\", parameter, \"as\", p)\n",
    "\n",
    "# p1 = P1('movie_3.csv')\n",
    "p1=P1('dependent_data.csv')\n",
    "p1.result(1)\n",
    "p1.result(2)\n",
    "p1.result(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2bed79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for UCB Algorithm:\n",
      "651.94 with parameter c as 1\n",
      "results for TS Algorithm:\n",
      "694.86 with parameter a,b as [[601, 401], [401, 601], [2, 3]]\n",
      "results for D-UCB Algorithm:\n",
      "701.22 with parameter c as 1\n",
      "results for TS-Gauss Algorithm:\n",
      "666.8 with parameter Beta as 1\n",
      "results for D-TS Algorithm:\n",
      "699.72 with parameter Beta as 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import operator\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Cython import wraparound, boundscheck\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cython.parallel as parallel\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, path):\n",
    "        self.data=None\n",
    "        self.table=None\n",
    "        self.arm_num=None\n",
    "        self.means=None\n",
    "        self.optArm=None\n",
    "        self.sampler=None\n",
    "        self.range=None\n",
    "        self.path=path\n",
    "        self.load_data(path)\n",
    "\n",
    "    def load_data(self,path):\n",
    "        self.data=pd.read_csv(path)\n",
    "        if 'arm1' in self.data.columns.values:\n",
    "            self.load_data_gen()\n",
    "        else:\n",
    "            self.load_data_movie()\n",
    "\n",
    "    def load_data_gen(self):\n",
    "        path=self.path\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.arm_num = self.data.columns.size\n",
    "        self.range=self.data[self.data.columns[0]].max()-self.data[self.data.columns[0]].min()\n",
    "        self.means = [0] * self.arm_num\n",
    "        for i in range(self.arm_num):\n",
    "            self.means[i] = self.data[self.data.columns[i]].mean()\n",
    "        self.optArm = np.argmax(self.means)\n",
    "\n",
    "        # 0,1,2,...\n",
    "        self.table = np.zeros([self.arm_num, self.data.values.max() + 1, self.arm_num])\n",
    "        for s_idx, source_arm in enumerate(self.data.columns):\n",
    "            for a_idx, aim_arm in enumerate(self.data.columns):\n",
    "                for i in range(self.data.values.max() + 1):\n",
    "                    if source_arm == aim_arm:\n",
    "                        self.table[s_idx][i][a_idx] = i\n",
    "                    else:\n",
    "                        p_reward = self.data[aim_arm][self.data[source_arm] == i].mean()\n",
    "                        if p_reward > 1:\n",
    "                            self.table[s_idx][i][a_idx] = 1\n",
    "                        else:\n",
    "                            self.table[s_idx][i][a_idx] = p_reward\n",
    "        self.sampler=self.sample_gen\n",
    "\n",
    "    def load_data_movie(self):\n",
    "        #csv data\n",
    "        self.arm_num=self.data['genre_col'].max()+1\n",
    "        self.range=self.data['Rating'].max()-self.data['Rating'].min()\n",
    "        self.means=[0]*self.arm_num\n",
    "        for i in range(self.arm_num):\n",
    "            self.means[i]=self.data[self.data['genre_col']==i]['Rating'].mean()\n",
    "        self.optArm=np.argmax(self.means)\n",
    "        #0,1,2,...\n",
    "        self.table = np.zeros([self.arm_num, self.data['Rating'].max()-self.data['Rating'].min()+1, self.arm_num])\n",
    "        for source in range(self.arm_num):\n",
    "            for score in range(self.data['Rating'].min(),self.data['Rating'].max()+1):\n",
    "                users=set(self.data[(self.data['genre_col'] == source) & (self.data['Rating'] == score)]['UserID'])\n",
    "                self.table[source][:,source]=np.arange(self.data['Rating'].min(), self.data['Rating'].max() + 1)\n",
    "                for aim in range(self.arm_num):\n",
    "                    if aim==source:\n",
    "                        continue\n",
    "                    temp=self.data[self.data['genre_col'] == aim & self.data['UserID'].isin(users)]['Rating']\n",
    "                    self.table[source][score-self.data['Rating'].min()][aim]=temp.mean()\n",
    "        self.sampler =self.sample_movie\n",
    "\n",
    "    def sample_movie(self,choose):\n",
    "        v= self.data[self.data['genre_col'] == choose]['Rating'].sample(n=1, replace=True)\n",
    "        return v.values[0]\n",
    "\n",
    "    def sample_gen(self, choose):\n",
    "        reward = self.data[self.data.columns[choose]].sample(n=1, replace=True)\n",
    "        return reward.values[0]\n",
    "\n",
    "actual_theta = [0.7, 0.5, 0.4]\n",
    "N = 1000\n",
    "trial_times = 50\n",
    "arms_part1 = [1, 2, 3]\n",
    "\n",
    "# GREEDY_epsilon = [0.1, 0.5, 0.9]\n",
    "UCB_c = [1]\n",
    "TS_ab = [[[601, 401], [401, 601], [2, 3]]]\n",
    "\n",
    "# greedy_regret = np.zeros(N+1)\n",
    "TS_regret = np.zeros(N+1)\n",
    "UCB_regret = np.zeros(N+1)\n",
    "dependent_UCB_regret = np.zeros(N+1)\n",
    "dependent_TS_regret = np.zeros(N+1)\n",
    "TS_Gauss_regret=np.zeros(N+1)\n",
    "class P1:\n",
    "\n",
    "    def __init__(self, depend_data_path=None):\n",
    "        self.path = depend_data_path\n",
    "        if depend_data_path is not None:\n",
    "            self.loader=Loader(depend_data_path)\n",
    "            self.actual_theta=self.loader.means\n",
    "            self.optArm=self.loader.optArm\n",
    "            self.arm_num=self.loader.arm_num\n",
    "            self.sampler=self.loader.sampler\n",
    "            self.table=self.loader.table\n",
    "            self.range=self.loader.range\n",
    "        else:\n",
    "            self.loader=None\n",
    "            self.table=None\n",
    "            self.range = 1\n",
    "            self.actual_theta=actual_theta\n",
    "            self.optArm=np.argmax(self.actual_theta)\n",
    "            self.arm_num = len(self.actual_theta)\n",
    "            self.sampler=self.independ_reward\n",
    "\n",
    "    def independ_reward(self, choose):\n",
    "        # choose: 1,2,3\n",
    "        probability = self.actual_theta[choose]\n",
    "        if random.uniform(0, 1) < probability:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def e_Greedy(self, N, epsilon):\n",
    "        # initialize\n",
    "        # Notice we set index start from 1\n",
    "        theta = [0]*self.arm_num\n",
    "        count =[0]*self.arm_num\n",
    "        total_reward = 0\n",
    "        greedy_current_regret = np.zeros(N+1)\n",
    "\n",
    "        for t in range(1, N + 1):\n",
    "            # I_t=1,2,3...\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                I_t = random.randint(0, self.arm_num-1)\n",
    "            else:\n",
    "                I_t = np.argmax(theta)\n",
    "                if I_t == 0:\n",
    "                    I_t = 1\n",
    "\n",
    "            count[I_t] += 1\n",
    "            r = self.sampler(I_t)\n",
    "            total_reward += r\n",
    "            theta[I_t] += (1 / count[I_t]) * (r - theta[I_t])\n",
    "            if t==0:\n",
    "                greedy_current_regret[t] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                greedy_current_regret[t] = greedy_current_regret[t-1] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        global greedy_regret\n",
    "        greedy_regret += greedy_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def Ucb(self, N, c):\n",
    "        # note the index start from 0\n",
    "        count = [0]* self.arm_num\n",
    "        theta = [0]* self.arm_num\n",
    "        total_reward = 0\n",
    "        UCB_current_regret = np.zeros(N+1)\n",
    "\n",
    "        # initialize\n",
    "        for t in range(self.arm_num):\n",
    "            I_t = t\n",
    "            count[I_t] = 1\n",
    "            theta[I_t] = self.sampler(I_t)\n",
    "            if t==0:\n",
    "                UCB_current_regret[t] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                UCB_current_regret[t] = UCB_current_regret[t-1] + actual_theta[self.optArm] - actual_theta[I_t]\n",
    "\n",
    "        for t in range(self.arm_num, N + 1):\n",
    "            # select and pull arm\n",
    "            I_t =np.argmax([theta[j] + c * math.sqrt(2 * math.log(t) / count[j]) for j in range(self.arm_num)])\n",
    "\n",
    "            count[I_t] += 1\n",
    "            r = self.sampler(I_t)\n",
    "            total_reward += r\n",
    "            theta[I_t] += (r - theta[I_t]) / count[I_t]\n",
    "            UCB_current_regret[t] = UCB_current_regret[t - 1] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        global UCB_regret\n",
    "        UCB_regret += UCB_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def depend_Ucb(self, N, c):\n",
    "        table=self.table\n",
    "        arm_num=self.arm_num\n",
    "\n",
    "        count = np.array([0]*arm_num)\n",
    "        theta = np.array([0.]*arm_num)\n",
    "        ucb_idx=dict(zip(range(arm_num),[np.inf]*arm_num))\n",
    "        ave_pseudo_reward=np.array([[np.inf]*arm_num]*arm_num)\n",
    "        sum_pseudo_reward=np.array([[0.]*arm_num]*arm_num)\n",
    "        d_UCB_current_regret = np.zeros(N+1)\n",
    "\n",
    "        total_reward = 0\n",
    "        for t in range(N):\n",
    "            if t<arm_num:\n",
    "                choose=t\n",
    "            else:\n",
    "                S_bool=(count>=(float(t-1)/arm_num))\n",
    "                k_emp_reward=np.max(theta[S_bool])\n",
    "                k_emp=np.where(theta==k_emp_reward)[0][0]\n",
    "                comp_set=set()\n",
    "                comp_set.add(k_emp)\n",
    "                min_phi = np.min(ave_pseudo_reward[:, S_bool], axis=1)\n",
    "                for k in range(arm_num):\n",
    "                    if min_phi[k]>= k_emp_reward:\n",
    "                        comp_set.add(k)\n",
    "\n",
    "                comp_idx={ind: ucb_idx[ind] for ind in comp_set}\n",
    "                choose=max(comp_idx.items(),key=operator.itemgetter(1))[0]\n",
    "            # print(t,choose)\n",
    "            if t==0:\n",
    "                d_UCB_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "            else:\n",
    "                d_UCB_current_regret[t+1] = d_UCB_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "\n",
    "            reward=self.sampler(choose)\n",
    "            count[choose]+=1\n",
    "            theta[choose]+=((reward-theta[choose])/count[choose])\n",
    "\n",
    "            for arm in range(arm_num):\n",
    "                if (count[arm] > 0):\n",
    "                    ucb_idx[arm] = theta[arm] + c * np.sqrt(2 * np.log(t + 1) / count[arm])\n",
    "\n",
    "            # pseudoReward=table[choose][reward]\n",
    "            pseudoReward = table[choose][reward - 1,:]\n",
    "            sum_pseudo_reward[:, choose] = sum_pseudo_reward[:, choose]+ pseudoReward\n",
    "            ave_pseudo_reward[:, choose] = np.divide(sum_pseudo_reward[:, choose], count[choose])\n",
    "\n",
    "            ave_pseudo_reward[np.arange(arm_num),np.arange(arm_num)]=theta\n",
    "\n",
    "            total_reward+=reward\n",
    "        global dependent_UCB_regret\n",
    "        dependent_UCB_regret += d_UCB_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def TS_sample_Gauss(self,theta,count,beta):\n",
    "        std=np.sqrt(float(beta)/(count+1))\n",
    "        return np.random.normal(theta,std)\n",
    "\n",
    "\n",
    "    def TS_arm_choose_beta(self, ab):\n",
    "        theta = [0 for i in ab]\n",
    "        for i, (a, b) in enumerate(ab):\n",
    "            theta[i] = np.random.beta(a, b)\n",
    "        return np.argmax(theta)\n",
    "\n",
    "    def TS(self, N, ab_original):\n",
    "\n",
    "        total_reward = 0\n",
    "        # ab idx start from 0\n",
    "        ab = copy.deepcopy(ab_original)\n",
    "        TS_current_regret = np.zeros(N+1)\n",
    "        for t in range(N):\n",
    "            I_t = self.TS_arm_choose_beta(ab)\n",
    "            # print(I_t)\n",
    "            # update distribution\n",
    "            r = self.sampler(I_t)\n",
    "            ab[I_t][0] += r\n",
    "            ab[I_t][1] += (1 - r)\n",
    "            total_reward += r\n",
    "            if t==0:\n",
    "                TS_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                TS_current_regret[t+1] = TS_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        # compute the expectation!!\n",
    "        # result = []\n",
    "        # for j in arms:\n",
    "        #     result.append(ab[j - 1][0] / (ab[j - 1][0] + ab[j - 1][1]))\n",
    "        # print(choose_first_cluster)\n",
    "        global TS_regret\n",
    "        TS_regret += TS_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    def TS_Gauss(self, N, beta):\n",
    "        arm_num=self.arm_num\n",
    "        total_reward = 0\n",
    "        # ab idx start from 0\n",
    "\n",
    "        TS_G_current_regret = np.zeros(N + 1)\n",
    "        theta = np.array([0.] * arm_num)\n",
    "        count = np.array([0] * arm_num)\n",
    "        for t in range(N):\n",
    "            I_t=np.argmax(self.TS_sample_Gauss(theta,count,beta))\n",
    "            # print(I_t)\n",
    "            # update distribution\n",
    "            r = self.sampler(I_t)\n",
    "            count[I_t] += 1\n",
    "            theta[I_t] += ((r - theta[I_t]) / count[I_t])\n",
    "            total_reward += r\n",
    "            if t==0:\n",
    "                TS_G_current_regret[t+1] = self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "            else:\n",
    "                TS_G_current_regret[t+1] = TS_G_current_regret[t] + self.actual_theta[self.optArm] - self.actual_theta[I_t]\n",
    "        # compute the expectation!!\n",
    "        # result = []\n",
    "        # for j in arms:\n",
    "        #     result.append(ab[j - 1][0] / (ab[j - 1][0] + ab[j - 1][1]))\n",
    "        # print(choose_first_cluster)\n",
    "        global TS_Gauss_regret\n",
    "        TS_Gauss_regret += TS_G_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    # TODO\n",
    "    def depend_TS(self, N, beta):\n",
    "        table = self.table\n",
    "        arm_num = self.arm_num\n",
    "\n",
    "        theta = np.array([0.] * arm_num)\n",
    "        count = np.array([0] * arm_num)\n",
    "\n",
    "        ave_pseudo_reward = np.array([[np.inf] * arm_num] * arm_num)\n",
    "        sum_pseudo_reward = np.array([[0.] * arm_num] * arm_num)\n",
    "        d_TS_current_regret = np.zeros(N + 1)\n",
    "\n",
    "        total_reward = 0\n",
    "        for t in range(N):\n",
    "            if t<arm_num:\n",
    "                choose=t\n",
    "            else:\n",
    "                S_bool = (count >= (float(t - 1) / arm_num))\n",
    "\n",
    "                k_emp_reward = np.max(theta[S_bool])\n",
    "                k_emp = np.where(theta == k_emp_reward)[0][0]\n",
    "                comp_set = set()\n",
    "                comp_set.add(k_emp)\n",
    "                min_phi = np.min(ave_pseudo_reward[:, S_bool], axis=1)\n",
    "                for k in range(arm_num):\n",
    "                    if min_phi[k] >= k_emp_reward:\n",
    "                        comp_set.add(k)\n",
    "\n",
    "                sample=self.TS_sample_Gauss(theta,count,beta)\n",
    "                comp_idx = {ind: sample[ind] for ind in comp_set}\n",
    "                choose = max(comp_idx.items(), key=operator.itemgetter(1))[0]\n",
    "            # print(t,choose)\n",
    "            if t == 0:\n",
    "                d_TS_current_regret[t + 1] = self.actual_theta[self.optArm] - self.actual_theta[choose]\n",
    "            else:\n",
    "                d_TS_current_regret[t + 1] = d_TS_current_regret[t] + self.actual_theta[self.optArm] - \\\n",
    "                                              self.actual_theta[choose]\n",
    "\n",
    "            reward = self.sampler(choose)\n",
    "            count[choose] += 1\n",
    "            theta[choose] += ((reward - theta[choose]) / count[choose])\n",
    "\n",
    "            # pseudoReward=table[choose][reward]\n",
    "            pseudoReward = table[choose][reward - 1, :]\n",
    "            sum_pseudo_reward[:, choose] = sum_pseudo_reward[:, choose] + pseudoReward\n",
    "            ave_pseudo_reward[:, choose] = np.divide(sum_pseudo_reward[:, choose], count[choose])\n",
    "\n",
    "            ave_pseudo_reward[np.arange(arm_num), np.arange(arm_num)] = theta\n",
    "\n",
    "            total_reward += reward\n",
    "        global dependent_TS_regret\n",
    "        dependent_TS_regret += d_TS_current_regret\n",
    "        return total_reward\n",
    "\n",
    "    @boundscheck(False)\n",
    "    @wraparound(False)\n",
    "    def result(self, function_idx):\n",
    "        function = ['epsilon-greedy', 'UCB', 'TS', 'D-UCB','TS-Gauss', 'D-TS']\n",
    "        print(\"results for\", function[function_idx - 1], \"Algorithm:\")\n",
    "        para = []\n",
    "        func = None\n",
    "        if function_idx == 1:\n",
    "            para = GREEDY_epsilon\n",
    "            func = self.e_Greedy\n",
    "            parameter = \"epsilon\"\n",
    "        elif function_idx == 2:\n",
    "            para = UCB_c\n",
    "            func = self.Ucb\n",
    "            parameter = \"c\"\n",
    "        elif function_idx == 3:\n",
    "            para = TS_ab\n",
    "            func = self.TS\n",
    "            parameter = \"a,b\"\n",
    "        elif function_idx == 4:\n",
    "            para = UCB_c\n",
    "            func = self.depend_Ucb\n",
    "            parameter = \"c\"\n",
    "        elif function_idx == 5:\n",
    "            para = [self.range]\n",
    "            func = self.TS_Gauss\n",
    "            parameter = \"Beta\"\n",
    "        elif function_idx == 6:\n",
    "            para = [self.range]\n",
    "            func = self.depend_TS\n",
    "            parameter = \"Beta\"\n",
    "\n",
    "        for p in para:\n",
    "            result = 0.0\n",
    "            for trial in parallel.prange(trial_times, nogil=True,schedule=\"static\", chunksize=1):\n",
    "                result += func(N, p)\n",
    "            result /= trial_times\n",
    "            print(result, \"with parameter\", parameter, \"as\", p)\n",
    "\n",
    "# p1 = P1('movie_3.csv')\n",
    "p1=P1('dependent_data.csv')\n",
    "# p1.result(1)\n",
    "p1.result(2)\n",
    "p1.result(3)\n",
    "p1.result(4)\n",
    "p1.result(5)\n",
    "p1.result(6)\n",
    "# proccessing data: taking the mean of regrets\n",
    "# greedy_regret /= trial_times\n",
    "TS_regret /= trial_times\n",
    "UCB_regret /= trial_times\n",
    "dependent_UCB_regret /= trial_times\n",
    "dependent_TS_regret /= trial_times\n",
    "TS_Gauss_regret/=trial_times\n",
    "\n",
    "# plot\n",
    "spacing = int(N/20)\n",
    "# plt.plot(range(0, N+1)[::spacing], greedy_regret[::spacing], label='epsilon-Greedy', color='black', marker='x')\n",
    "plt.plot(range(0, N+1)[::spacing], UCB_regret[::spacing], label='UCB', color='red', marker='+')\n",
    "plt.plot(range(0, N+1)[::spacing], TS_regret[::spacing], label='TS', color='yellow', marker='o')\n",
    "plt.plot(range(0, N+1)[::spacing], dependent_UCB_regret[::spacing], label='D-UCB', color='blue', marker='^')\n",
    "plt.plot(range(0, N+1)[::spacing], dependent_TS_regret[::spacing], label='D-TS', color='green', marker='^')\n",
    "plt.plot(range(0, N+1)[::spacing], TS_Gauss_regret[::spacing], label='TS-Gauss', color='purple', marker='^')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.xlabel('Number of Rounds')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ce475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
